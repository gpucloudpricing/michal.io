---
time_modified: 2024-11-03T15:34:21-04:00
time_created: 2023-12-09T00:06:17-05:00
---

![[Pasted image 20241107101436.png]]

- [retnet](https://github.com/microsoft/unilm/tree/master/retnet)

- [GitHub - BlinkDL/RWKV-LM: RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx\_len, and free sentence embedding.](https://github.com/BlinkDL/RWKV-LM)
	- [GitHub - BlinkDL/ChatRWKV: ChatRWKV is like ChatGPT but powered by RWKV (100% RNN) language model, and open source.](https://github.com/BlinkDL/ChatRWKV)

- [[2306.11197] Sparse Modular Activation for Efficient Sequence Modeling](https://arxiv.org/abs/2306.11197)
- [GitHub - renll/SeqBoat: [NeurIPS 2023] Sparse Modular Activation for Efficient Sequence Modeling](https://github.com/renll/SeqBoat)


## RWKV

- [ ] [RWKV-LM/RWKV-v7 at main 路 BlinkDL/RWKV-LM 路 GitHub](https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7)


## Monarch Mixer
[Making Foundation Models More Efficient - Dan Fu | Stanford MLSys #86 - YouTube](https://www.youtube.com/watch?v=IS59IwGLvVs)



## Mamba
- [ ] [Mamba: The Hard Way](https://srush.github.io/annotated-mamba/hard.html)

## Mamba2


## Codestral Mamba (Mistral)

## H3

## Zamba

## Zamba2
- [ ] [Zyphra](https://www.zyphra.com/post/zamba2-7b)
![[Pasted image 20241014203628.png]]
![[Pasted image 20241127180930.png]]

[YouTube](https://youtu.be/xFU7mfBfVSw?si=UUKjnKC2JSb5v38C)
## Jamba


## Samba
- [ ] [GitHub - microsoft/Samba: Official implementation of "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"](https://github.com/microsoft/Samba)

## minGRU and minLSTM
- [ ] [\[2410.01201\] Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201)


## xLSTM


## Hawk and Griffin
- [ ] [Fetching Title#mdig](https://www.youtube.com/watch?v=GfAT2zkB6-U)
- [ ] https://www.youtube.com/watch?v=0Yi3yUjB-3M&list=PPSV

![[Pasted image 20241106014030.png]]


![[Pasted image 20241106014540.png]]


## DeltaNet

## Gated DeltaNets

![[Screenshot 2024-11-03 at 3.28.16 PM.png]]



## Hymba
- [ ] [\[2411.13676\] Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676)
[nvidia/Hymba-1.5B-Base 路 Hugging Face](https://huggingface.co/nvidia/Hymba-1.5B-Base)

- [ ] [modeling\_hymba.py 路 nvidia/Hymba-1.5B-Base at main](https://huggingface.co/nvidia/Hymba-1.5B-Base/blob/main/modeling_hymba.py)

![[Pasted image 20241125154526.png]]

![[Pasted image 20241125155231.png]]
- [ ] claims parallel heads work better than stacking
- [ ] adds register tokens (128)
	- [ ] 
- [ ] uses KV cache sharing across layers



## Attamba

- [ ] [\[2411.17685\] Attamba: Attending To Multi-Token States](https://arxiv.org/abs/2411.17685)