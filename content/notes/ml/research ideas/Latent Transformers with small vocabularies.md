---
time_modified: 2024-10-11T11:36:58-04:00
time_created: 2024-10-09T12:31:25-04:00
---


Use small vocab (or even character level) (ex 1024) with a 1D causal conv VAE to reduce embedding table size and sequence lengths


align from small token set to existing large tokenizers with CTC? ([Sequence Modeling with CTC](https://distill.pub/2017/ctc/))

## Related Work
- [ ] [\[2305.07185\] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)
