---
time_modified: 2024-10-23T22:21:43-04:00
time_created: 2024-10-09T12:36:37-04:00
---

study ways to get rid of more modules in transformers


bounded activations to remove normalization layers

get rid of softmax and sequence level compute everywhere (ex: siglip and sigmoid attention)

don't normalize 





- [ ] [\[2406.15786\] What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)
- [ ] [\[2407.15516\] Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models](https://arxiv.org/abs/2407.15516)
- [ ] [SKIP-ATTENTION: IMPROVING VISION TRANSFORMERS BY PAYING LESS ATTENTION](https://openreview.net/pdf?id=vI95kcLAoU)