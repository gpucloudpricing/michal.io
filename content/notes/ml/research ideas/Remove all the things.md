---
time_modified: 2024-10-09T16:41:06-04:00
time_created: 2024-10-09T12:36:37-04:00
---

study ways to get rid of more modules in transformers


bounded activations to remove normalization layers

get rid of softmax and sequence level compute everywhere (ex: siglip and sigmoid attention)

don't normalize 