# Server Inference

- [ ] [GitHub - sgl-project/sglang: SGLang is yet another fast serving framework for large language models and vision language models.](https://github.com/sgl-project/sglang/tree/main)
	- [ ] [GitHub - sgl-project/sgl-learning-materials: Materials for learning SGLang](https://github.com/sgl-project/sgl-learning-materials)

- [ ] [GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)


## Triton-inference-server

- [Serving a Torch-TensorRT model with Triton â€” Torch-TensorRT v1.4.0.dev0+d0af394 documentation](https://pytorch.org/TensorRT/tutorials/serving_torch_tensorrt_with_triton.html#serving-torch-tensorrt-with-triton)


## ONNX Runtime

https://github.com/microsoft/onnxruntime-inference-examples



https://github.com/microsoft/DeepSpeed-MII



https://github.com/microsoft/onnx-script




[GitHub - open-mmlab/mmdeploy: OpenMMLab Model Deployment Framework](https://github.com/open-mmlab/mmdeploy)