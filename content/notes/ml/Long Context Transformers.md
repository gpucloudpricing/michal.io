---
time_modified: 2024-10-30T15:46:52-04:00
time_created: 2024-09-25T20:58:02-04:00
---
#transformers 

- [ ] [Long-Context LLM Extension - YouTube](https://www.youtube.com/watch?v=dc4chADushM)
- [ ] [Everything About Long Context Fine-tuning](https://huggingface.co/blog/wenbopan/long-context-fine-tuning)

- [ ] [\[2309.12307\] LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
- [ ] [YouTube](https://www.youtube.com/live/gVeh6qzcGOc?si=gF71OxRao4WdDa-6)

## RoPE Expansion

- [ ] [Gradient Blog: Scaling Rotational Embeddings for Long-Context Language Models](https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models)
	- [ ] rope theta scaling + fine tuning on longer context data
- [ ] [Extending the RoPE | EleutherAI Blog](https://blog.eleuther.ai/yarn/)


## Sliding Window Attention



## Ring Attention

- [ ]  [RING Attention explained: 1 Mio Context Length - YouTube](https://www.youtube.com/watch?v=jTJcP8iyoOM)

### Tree Attention
- [ ] [\[2408.04093\] Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters](https://arxiv.org/abs/2408.04093)

## StreamingLLM

- [ ] [\[2309.17453\] Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
	- [ ] [GitHub - mit-han-lab/streaming-llm: \[ICLR 2024\] Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)
- [ ] [StreamingLLM - Efficient Streaming Language Models with Attention Sinks Explained - YouTube](https://www.youtube.com/watch?v=f23sUViqxH8)

![[Pasted image 20241003221622.png]]

![[Pasted image 20241003221906.png]]


[GitHub - princeton-nlp/ProLong: Homepage for ProLong (Princeton long-context language models) and paper "How to Train Long-Context Language Models (Effectively)"](https://github.com/princeton-nlp/ProLong)

[x.com](https://x.com/_awettig/status/1842236764607431110)
[x.com](https://x.com/_awettig/status/1842236764607431110)



## Longformer

## Linformer

## Reformer

## Blockwise Attention

## Adaptive Attention Span


## Infini-Attention
- [ ] [A failed experiment: Infini-Attention, and why we should keep trying?](https://huggingface.co/blog/infini-attention)