#llm

- [ ] [\[2406.16838\] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models](https://arxiv.org/abs/2406.16838)
- [ ] [GitHub - shreyansh26/LLM-Sampling: A collection of various LLM sampling methods implemented in pure Pytorch](https://github.com/shreyansh26/LLM-Sampling)
- [ ] [[Test Time Compute and LLM Reasoning]]
- [ ] [Text generation strategies](https://huggingface.co/docs/transformers/v4.47.0/en/generation_strategies)






- [ ] [\[2402.10200\] Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200)
- [ ] [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/html/2402.12875v3#abstract)
# Greedy
- pick top result at each steps


# Top K Sampling
- sample from top K 


# Top P Sampling / Nucleus Sampling
- sample from the top tokens that add up to P


# Beam Search


# Speculative


# Structured

[[Structured Generation with LLMs]]

# MCTS