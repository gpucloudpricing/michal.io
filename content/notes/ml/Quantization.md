---
time_modified: 2024-10-13T02:32:00-04:00
time_created: 2024-09-26T00:00:18-04:00
---
- [ ] [GitHub - bitsandbytes-foundation/bitsandbytes: Accessible large language models via k-bit quantization for PyTorch.](https://github.com/bitsandbytes-foundation/bitsandbytes)
- [ ] [GitHub - pytorch/ao: PyTorch native quantization and sparsity for training and inference](https://github.com/pytorch/ao)
- [ ] [GitHub - intel/auto-round: Advanced Quantization Algorithm for LLMs. This is official implementation of "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"](https://github.com/intel/auto-round)
- [ ] [GitHub - ChenMnZ/PrefixQuant: An algorithm for static activation quantization of LLMs](https://github.com/ChenMnZ/PrefixQuant)

## Training

- [ ] [Lecture 30: Quantized Training - YouTube](https://www.youtube.com/watch?v=Br07GsnnvWc)
![[Screenshot 2024-10-07 at 7.44.10 PM.png]]![[Screenshot 2024-10-07 at 7.47.46 PM.png]]![[Screenshot 2024-10-07 at 7.49.07 PM.png]]
![[Pasted image 20241007204155.png]]

### Stochastic Rounding

