---
time_modified: 2024-10-06T15:17:04-04:00
time_created: 2024-09-29T16:16:27-04:00
draft: false
---

## Models
- [ ] [SAM 2.1 with training code](https://github.com/facebookresearch/sam2)
	- [ ] [x.com](https://x.com/nikhilaravi/status/1840847032371560520)
- [ ] [Liquid Foundation Models: Our First Series of Generative AI Models](https://www.liquid.ai/liquid-foundation-models)
- [ ] [GitHub - THUDM/CogView3: text to image to  generation: CogView3-Plus and CogView3(ECCV 2024)](https://github.com/THUDM/CogView3)
- [ ] [Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/)
- [ ] [vidore/colqwen2-v0.1 · Hugging Face](https://huggingface.co/vidore/colqwen2-v0.1)
- [ ] [Announcing FLUX1.1 \[pro\] and the BFL API - Black Forest Labs](https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/)
## Papers
- [ ] [\[2409.18869v1\] Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869v1)
- [ ] [\[2409.17692\] MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692)
- [ ] [\[2405.03882\] Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer](https://arxiv.org/abs/2405.03882)
- [ ] [\[2409.16280\] MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280)
- [ ] [Site Unreachable](https://arxiv.org/abs/2409.20566)
- [ ] [\[2409.20370\] The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/abs/2409.20370)
- [ ] [\[2408.05088\] UNIC: Universal Classification Models via Multi-teacher Distillation](https://arxiv.org/abs/2408.05088)
- [ ] [\[2407.09111\] Inference Optimization of Foundation Models on AI Accelerators](https://arxiv.org/abs/2407.09111)
- [ ] [\[2402.10376\] Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)](https://arxiv.org/abs/2402.10376)
	- [ ] [x.com](https://x.com/alex_oesterling/status/1841592274553057485)
- [ ] [\[2410.01806v1\] Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking](https://arxiv.org/abs/2410.01806v1)
- [ ] [\[2410.01201\] Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201) #rnn 
- [ ] [\[2410.01679\] VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment](https://arxiv.org/abs/2410.01679)
- [ ] https://arxiv.org/abs/2409.20370
- [ ] [Movie Gen: A Cast of Media Foundation Models](https://ai.meta.com/static-resource/movie-gen-research-paper)
- [ ] [\[2410.02746\] Contrastive Localized Language-Image Pre-Training](https://arxiv.org/abs/2410.02746)
## Code
- [ ] [GitHub - THUDM/SwissArmyTransformer: SwissArmyTransformer is a flexible and powerful library to develop your own Transformer variants.](https://github.com/THUDM/SwissArmyTransformer)
- [ ] [x.com](https://x.com/FerdinandMom/status/1841419180702351369)
	- [ ] [Self contained example of how pipeline parallel works (AFAB and 1F1B) in 200 LOC · GitHub](https://gist.github.com/3outeille/a3d4d91bb07af64c8f33d5aaee5145fe) 
- [ ] [GitHub - evanatyourservice/kron\_torch: An implementation of PSGD Kron second-order optimizer for PyTorch](https://github.com/evanatyourservice/kron_torch)

## Articles
- [ ] [Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation) | by Lucas de Lima Nogueira | Towards Data Science](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc)
- [ ] [Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/)
- [ ] [Cheng Luo - MINI-SEQUENCE TRANSFORMER (MST)](https://wdlctc.github.io/mst.html)
- [ ] [Distributed Training Of Deep Learning Models : Part \~ 1](https://vaibhawvipul.github.io/2024/09/29/Distributed-Training-of-Deep-Learning-models-Part-~-1.html) #distributed
- [ ] [How to train a model on 10k H100 GPUs?](https://soumith.ch/blog/2024-10-02-training-10k-scale.md.html) #distributed 
- [ ] [Self contained example of how pipeline parallel works (AFAB and 1F1B) in 200 LOC · GitHub](https://gist.github.com/3outeille/a3d4d91bb07af64c8f33d5aaee5145fe)
- [ ] [Deploy SkyPilot on existing machines — SkyPilot documentation](https://skypilot.readthedocs.io/en/latest/reservations/existing-machines.html) #skypilot
- [ ] [Transformers Inference Optimization Toolset | AstraBlog](https://astralord.github.io/posts/transformer-inference-optimization-toolset/)

## Videos
- [ ] [PyTorch Conference 2024 - YouTube](https://www.youtube.com/playlist?list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l)
- [ ] [2024 RL Conference ](https://youtube.com/playlist?list=PLEA9Mnr-L18lI_I-EkyAc1-gXgBj52oV5&si=nHmif-Q7iD1gu2u2) #rl
- [ ] [Modern GPU Architecture](https://youtu.be/whPSD8sdx-0?si=PBPU2woJPHaf0E8n)

## Other
- [ ] ECCV
	- [ ] [ILR2024](https://ilr-workshop.github.io/ECCVW2024/) instance level recognition
- [ ] [litellm/model\_prices\_and\_context\_window.json at main · BerriAI/litellm · GitHub](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json) LLM specs
- [ ] [Llm Pricing - a Hugging Face Space by philschmid](https://huggingface.co/spaces/philschmid/llm-pricing)