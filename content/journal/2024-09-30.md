---
time_modified: 2024-10-01T22:49:41-04:00
time_created: 2024-09-29T16:16:27-04:00
draft: false
---

## Models
- [ ] [SAM 2.1 with training code](https://github.com/facebookresearch/sam2)
	- [ ] [x.com](https://x.com/nikhilaravi/status/1840847032371560520)
- [ ] [Liquid Foundation Models: Our First Series of Generative AI Models](https://www.liquid.ai/liquid-foundation-models)
- [ ] [GitHub - THUDM/CogView3: text to image to  generation: CogView3-Plus and CogView3(ECCV 2024)](https://github.com/THUDM/CogView3)
- [ ] [Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/)
- [ ] [vidore/colqwen2-v0.1 · Hugging Face](https://huggingface.co/vidore/colqwen2-v0.1)
## Papers
- [ ] [\[2409.18869v1\] Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869v1)
- [ ] [\[2409.17692\] MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692)
- [ ] [\[2405.03882\] Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer](https://arxiv.org/abs/2405.03882)
- [ ] [\[2409.16280\] MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280)
- [ ] [Site Unreachable](https://arxiv.org/abs/2409.20566)
- [ ] [\[2409.20370\] The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/abs/2409.20370)

## Code
- [ ] [GitHub - THUDM/SwissArmyTransformer: SwissArmyTransformer is a flexible and powerful library to develop your own Transformer variants.](https://github.com/THUDM/SwissArmyTransformer)

## Articles
- [ ] [Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation) | by Lucas de Lima Nogueira | Towards Data Science](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc)
- [ ] [Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/)
- [ ] [Cheng Luo - MINI-SEQUENCE TRANSFORMER (MST)](https://wdlctc.github.io/mst.html)

## Videos
- [ ] [PyTorch Conference 2024 - YouTube](https://www.youtube.com/playlist?list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l)

## Other
- [ ] ECCV
	- [ ] [ILR2024](https://ilr-workshop.github.io/ECCVW2024/) instance level recognition
- [ ] [litellm/model\_prices\_and\_context\_window.json at main · BerriAI/litellm · GitHub](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json) LLM specs