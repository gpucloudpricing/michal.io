---
time_modified: 2024-10-01T23:27:25-04:00
time_created: 2024-09-20T09:35:18-04:00
---

## Models
- [ ] [\[2409.11402\] NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/abs/2409.11402) #vlm
	- [ ] [GitHub - NVIDIA/Megatron-Energon: Megatron's multi-modal data loader](https://github.com/NVIDIA/Megatron-Energon/tree/develop)
- [ ] [Qwen2.5: A Party of Foundation Models! | Qwen](https://qwenlm.github.io/blog/qwen2.5/)
- [ ] [GitHub - ictnlp/LLaMA-Omni: LLaMA-Omni is a low-latency and high-quality end-to-end speech interaction model built upon Llama-3.1-8B-Instruct, aiming to achieve speech capabilities at the GPT-4o level.](https://github.com/ictnlp/LLaMA-Omni) #speech
- [ ] [GitHub - kyutai-labs/moshi](https://github.com/kyutai-labs/moshi) #speech
	- [ ] [Moshi.pdf](https://kyutai.org/Moshi.pdf)
- [ ] [GitHub - microsoft/GRIN-MoE](https://github.com/microsoft/GRIN-MoE)
## Papers
- [ ] [EUREKA: Evaluating and Understanding Large Foundation Models - Microsoft Research](https://www.microsoft.com/en-us/research/publication/eureka-evaluating-and-understanding-large-foundation-models/)
	- [ ] [GitHub - microsoft/eureka-ml-insights: A framework for standardizing evaluations of large foundation models, beyond single-score reporting and rankings.](https://github.com/microsoft/eureka-ml-insights)
- [ ] [\[2409.11321\] SOAP: Improving and Stabilizing Shampoo using Adam](https://arxiv.org/abs/2409.11321) #optimizers 
	- [ ] [GitHub - nikhilvyas/SOAP](https://github.com/nikhilvyas/SOAP/tree/main)
- [ ] [\[2409.10173\] jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173) #text-embeddings
- [ ] [\[2409.12191\] Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
- [ ] [\[2409.11564\] Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey](https://arxiv.org/abs/2409.11564)

## Code
- [ ] [GitHub - kyleliang919/Online-Subspace-Descent: This repo is based on https://github.com/jiaweizzhao/GaLore, paper coming soon](https://github.com/kyleliang919/Online-Subspace-Descent) #optimizers #compression
- [ ] [GitHub - NVIDIA/Megatron-Energon: Megatron's multi-modal data loader](https://github.com/NVIDIA/Megatron-Energon/tree/develop) #multimodal #dataloader #pytorch
- [ ] [GitHub - TorchDR/TorchDR: TorchDR - PyTorch Dimensionality Reduction](https://github.com/TorchDR/TorchDR/tree/main) #pytorch #umap #tsne
- [ ] [GitHub - modelscope/ms-swift: Use PEFT or Full-parameter to finetune 350+ LLMs or 90+ MLLMs. (Qwen2, GLM4v, Internlm2.5, Yi, Llama3.1, Llava-Video, Internvl2, MiniCPM-V-2.6, Deepseek, Baichuan2, Gemma2, Phi3-Vision, ...)](https://github.com/modelscope/ms-swift) #vision-language #vlm #tuning
- [ ] [Release v0.3.0 Release Note · linkedin/Liger-Kernel · GitHub](https://github.com/linkedin/Liger-Kernel/releases/tag/v0.3.0)
- [ ] [GitHub - voideditor/void](https://github.com/voideditor/void) open source cursor alternative
- [ ] [GitHub - pytorch-labs/LeanRL: LeanRL is a fork of CleanRL, where selected PyTorch scripts optimized for performance using compile and cudagraphs.](https://github.com/pytorch-labs/LeanRL) #pytorch #rl

## Articles
- [ ] [Improved Data Loading with Threads | NVIDIA Technical Blog](https://developer.nvidia.com/blog/improved-data-loading-with-threads/)
	- [ ] uses noGIL mode to evaluate a threaded torch DataLoader
	- [ ] CUDA context switches a larger issue with process based workers
	- [ ] no difference in Pillow
- [ ] [rerankers: A Lightweight Python Library to Unify Ranking Methods – Answer.AI](https://www.answer.ai/posts/2024-09-16-rerankers.html)
- [ ] [\[Distributed w/ TorchTitan\] Introducing Async Tensor Parallelism in PyTorch - distributed / torchtitan - PyTorch Forums](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487/1) #pytorch #distributed
- [ ] [Polars — GPU acceleration with Polars and NVIDIA RAPIDS](https://pola.rs/posts/gpu-engine-release/) #tabular
- [ ] [How to make LLMs go fast](https://vgel.me/posts/faster-inference/)
- [ ] [Fine-tuning LLMs to 1.58bit: extreme quantization made easy](https://huggingface.co/blog/1_58_llm_extreme_quantization)
- [ ] [static.sched.com/hosted\_files/pytorch2024/8f/Pytorch Conference - Making LLM training faster.pdf](https://static.sched.com/hosted_files/pytorch2024/8f/Pytorch%20Conference%20-%20Making%20LLM%20training%20faster.pdf)
- [ ] [Inference-Friendly Models with MixAttention | Databricks Blog](https://www.databricks.com/blog/mixattention)
- [ ] [Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/)
- [ ] https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html

## Videos
- [ ] [CS 194/294-196 (LLM Agents) - Lecture 1 - YouTube](https://www.youtube.com/live/QL-FS_Zcmyo?si=cOJ2ehYvXiKc12ey)
	- [ ] [Large Language Model Agents](https://llmagents-learning.org/f24)
- [ ] [YouTube](https://youtu.be/CRpHNB87gRY?si=bDc_qDqbVbKW6aGW) twiml Simon Williamson video about LLMs for code
- [ ] [YouTube](https://youtu.be/eaAonE58sLU?si=s0weM1yKvKm7x9AP) Noam Brown from OpenAI on test time compute and planning
- [ ] [Tabular Learning: skrub and Foundation Models with Gaël Varoquaux, PhD - YouTube](https://www.youtube.com/watch?v=ITC_rmo3rhc) #tabular

## Other
- [ ] [Illuminate](https://illuminate.google.com/papers) - paper to podcast tool from google
	- [ ] [Apple last week gave us "Sigmoid Self-Attention" - Paper Podcast - YouTube](https://www.youtube.com/watch?v=qmm6GhA5w3Y)


