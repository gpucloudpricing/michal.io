---
time_modified: 2024-11-05T09:19:25-05:00
time_created: 2024-09-10T07:26:54-04:00
---

## Models
- [ ] 
## Papers
- [ ] [\[2410.19456\] Computational Bottlenecks of Training Small-scale Large Language Models](https://arxiv.org/abs/2410.19456)
- [ ] [\[2410.19313\] COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://arxiv.org/abs/2410.19313)
- [ ] [\[2410.19324\] Simpler Diffusion (SiD2): 1.5 FID on ImageNet512 with pixel-space diffusion](https://arxiv.org/abs/2410.19324)
- [ ] [\[2410.18111\] Data Efficiency for Large Recommendation Models](https://arxiv.org/abs/2410.18111)
- [ ] [\[2410.16048\] Continuous Speech Synthesis using per-token Latent Diffusion](https://arxiv.org/abs/2410.16048)
- [ ] [\[2410.18558\] Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data](https://arxiv.org/abs/2410.18558)
- [ ] [\[2410.18613\] Rethinking Softmax: Self-Attention with Polynomial Activations](https://arxiv.org/abs/2410.18613)
- [ ] [\[2410.17980\] Stick-breaking Attention](https://arxiv.org/abs/2410.17980)
- [ ] [Towards Learning to Reason at Pre-Training Scale | OpenReview](https://openreview.net/forum?id=BGnm7Lo8oW)
- [ ] [HIL-SERL: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning](https://hil-serl.github.io/)
- [ ] [\[2410.18779\] A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs](https://arxiv.org/abs/2410.18779)
- [ ] [\[2410.20672\] Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA](https://arxiv.org/abs/2410.20672)
- [ ] [\[2410.20280\] MarDini: Masked Autoregressive Diffusion for Video Generation at Scale](https://arxiv.org/abs/2410.20280)
- [ ] [\[2410.21465\] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
- [ ] [\[2405.02793\] ImageInWords: Unlocking Hyper-Detailed Image Descriptions](https://arxiv.org/abs/2405.02793)
- [ ] [\[2406.11832\] Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/abs/2406.11832)
- [ ] [\[2410.20399\] ThunderKittens: Simple, Fast, and Adorable AI Kernels](https://arxiv.org/abs/2410.20399)
	[GitHub - HazyResearch/ThunderKittens: Tile primitives for speedy kernels](https://github.com/HazyResearch/ThunderKittens)
- [ ] [GitHub - multimodal-interpretability/nnn: Nearest Neighbor Normalization (EMNLP 2024)](https://github.com/multimodal-interpretability/nnn)
- [ ] [pi0.pdf](https://www.physicalintelligence.company/download/pi0.pdf)
- [ ] [\[2410.22179\] Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech](https://arxiv.org/abs/2410.22179)
- [ ] [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing | OpenReview](https://openreview.net/forum?id=4D0f16Vwc3)
## Code
- [ ] 

## Articles
- [ ] [Our First Generalist Policy](https://www.physicalintelligence.company/blog/pi0)

## Videos
- [ ] [Guest Lecture by Kylo Lo: Demystifying data curation for pretrained language models - YouTube](https://www.youtube.com/watch?v=AlRQWKOQfbY)
- [ ] [SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models - YouTube](https://www.youtube.com/watch?v=K8iqzH5pKkQ)
- [ ] [Cohere For AI - Community Talks: Randall Balestriero - YouTube](https://www.youtube.com/watch?v=YRYNSlssDZU)
- [ ] [Training Zamba: A Hybrid Model Master Class with Zyphra's Quentin Anthony - YouTube](https://www.youtube.com/watch?v=xFU7mfBfVSw)

## Other
- [ ]


## Tweets


![](https://x.com/jxmnop/status/1851706815244902691)


![](https://x.com/EranMalach/status/1850885792836861966)



![](https://x.com/emiel_hoogeboom/status/1850879641877135443)

![](https://x.com/raymin0223/status/1851216039822180759)

## Notes

- [ ] Weight sharing for attention layers
	- [ ] Compute looks the same, tie weights to limit parameter counts
- [ ] SSM + Attention Hybrids
	- [ ] Can drop most attention layers and use SSMs in their place
	- [ ] Sliding window attention + Mamba (from Samba)