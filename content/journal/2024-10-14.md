---
time_modified: 2024-10-23T22:21:26-04:00
time_created: 2024-09-10T07:26:54-04:00
---

## Models
- [ ] [Introducing Play 3.0 Mini - A Lightweight, Reliable And Cost-efficient Multilingual Text-to-Speech Model](https://play.ht/news/introducing-play-3-0-mini/)
- [ ] [üçì Ichigo: Llama learns to talk - Homebrew](https://homebrew.ltd/blog/llama-learns-to-talk)
- [ ] - [Zyphra on X: "Today, in collaboration with @NvidiaAI, we bring you Zamba2-7B ‚Äì a hybrid-SSM model that outperforms Mistral, Gemma, Llama3 &amp; other leading models in both quality and speed. Zamba2-7B is the leading model for ‚â§8B weight class. üëáSee more in the thread belowüëá https://t.co/v1PttlaZq5" / X](https://x.com/ZyphraAI/status/1845939850958327822)
- [ ] [nvidia/Llama-3.1-Nemotron-70B-Instruct ¬∑ Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct)
- [ ] [deepseek-ai/Janus-1.3B ¬∑ Hugging Face](https://huggingface.co/deepseek-ai/Janus-1.3B)
## Papers
- [ ] [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://sihyun.me/REPA/)
- [ ] [\[2410.10819\] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819)
- [ ] [\[2410.02367\] SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367)
	- [ ] [GitHub - thu-ml/SageAttention: Quantized Attention that achieves speedups of 2.1x and 2.7x compared to FlashAttention2 and xformers, respectively, without lossing end-to-end metrics across various models.](https://github.com/thu-ml/SageAttention)
- [ ] [\[2410.06511v1\] TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://arxiv.org/abs/2410.06511v1)
- [ ] [\[2410.10630\] Thinking LLMs: General Instruction Following with Thought Generation](https://arxiv.org/abs/2410.10630)
- [ ] [Sana - Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer](https://nvlabs.github.io/Sana/)
- [ ] [\[2410.07815\] Simple ReFlow: Improved Techniques for Fast Flow Models](https://arxiv.org/abs/2410.07815)
- [ ] [\[2410.10733v1\] Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models](https://arxiv.org/abs/2410.10733v1)

## Code
- [ ] [GitHub - AI-Hypercomputer/maxtext: A simple, performant and scalable Jax LLM!](https://github.com/AI-Hypercomputer/maxtext/tree/main)
- [ ] [GitHub - mit-han-lab/efficientvit: Efficient vision foundation models for high-resolution generation and perception.](https://github.com/mit-han-lab/efficientvit)

## Articles
- [ ] [Linearizing LLMs with LoLCATs](https://www.together.ai/blog/linearizing-llms-with-lolcats)
- [ ] [Decentralized Training of Deep Learning Models](https://vaibhawvipul.github.io/2024/10/15/Decentralized-Training-of-Deep-Learning-Models.html)
- [ ] [INTELLECT‚Äì1: Launching the First Decentralized Training of a 10B Parameter Model](https://www.primeintellect.ai/blog/intellect-1)
- [ ] [Bug Fixes in LLM Training - Gradient Accumulation](https://unsloth.ai/blog/gradient)

## Videos
- [ ] [On the Tradeoffs of State Space Models - YouTube](https://www.youtube.com/watch?v=ksRp_DIHWj4)
- [ ] [Sam Smith - How to train an LLM - IPAM at UCLA - YouTube](https://www.youtube.com/watch?v=GfAT2zkB6-U)
- [ ] [Keynote: Yann LeCun, "Human-Level AI" - YouTube](https://www.youtube.com/watch?v=4DsCtgtQlZU)

## Other
- [ ]


## Tweets



## Notes


### [Sam Smith - How to train an LLM](https://www.youtube.com/watch?v=GfAT2zkB6-U)

![[Screenshot 2024-10-15 at 8.08.37 PM.png]]

![[Pasted image 20241015201014.png]]

Gated MLPs


![[Screenshot 2024-10-15 at 8.12.29 PM.png]]![[Screenshot 2024-10-15 at 8.16.19 PM.png]]

![[Pasted image 20241015201732.png]]

![[Pasted image 20241015201850.png]]

![[Pasted image 20241015201935.png]]![[Screenshot 2024-10-15 at 8.20.43 PM.png]]
![[Pasted image 20241015202107.png]]
![[Pasted image 20241015202131.png]]![[Screenshot 2024-10-15 at 8.23.29 PM.png]]

![[Pasted image 20241015202642.png]]

![[Pasted image 20241015202852.png]]![[Screenshot 2024-10-15 at 8.31.01 PM.png]]![[Screenshot 2024-10-15 at 8.32.43 PM.png]]![[Screenshot 2024-10-15 at 8.34.31 PM.png]]![[Screenshot 2024-10-15 at 8.41.02 PM.png]]![[Screenshot 2024-10-15 at 8.41.22 PM.png]]
![[Screenshot 2024-10-15 at 8.43.07 PM.png]]![[Screenshot 2024-10-15 at 8.44.54 PM.png]]![[Screenshot 2024-10-15 at 8.45.36 PM.png]]![[Screenshot 2024-10-15 at 8.47.02 PM.png]]