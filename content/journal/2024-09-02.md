

## Papers
- [ ] [\[2404.16710\] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710) #transformers
- [ ] [\[2405.04434\] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)
	- [ ] ![[Pasted image 20240904210108.png]]
- [ ] [\[2409.02060\] OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)
## Models

- [ ] OLMoE - 1B Mixture of Experts #moe 
	- [ ] [\[2409.02060\] OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)
	- [ ] [Niklas Muennighoff on X: "Releasing OLMoE - the first good Mixture-of-Experts LLM that's 100% open-source - 1B active, 7B total params for 5T tokens - Best small LLM &amp; matches more costly ones like Gemma, Llama - Open Model/Data/Code/Logs + lots of analysis &amp; experiments ðŸ“œhttps://t.co/Vpac2q90CS ðŸ§µ1/9 https://t.co/YOMV5t2Td1" / X](https://x.com/Muennighoff/status/1831159130230587486)
	- [ ] - [Niklas Muennighoff on X: "OLMoE Efficiency Thanks to Mixture-of-Experts, better data &amp; hyperparams, OLMoE is much more efficient than OLMo 7B: - &gt;4x less training FLOPs - &gt;5x less params used per forward pass i.e. cheaper training + cheaper inference! ðŸ§µ3/9 https://t.co/as9CNMTw9j" / X](https://x.com/Muennighoff/status/1831159133795647829)
- [ ] [Meet Yi-Coder: A Small but Mighty LLM for Code - 01.AI Blog](https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md) #code-model

- [ ] Reflection 70B
	- [ ] [Matt Shumer on X: "The technique that drives Reflection 70B is simple, but very powerful. Current LLMs have a tendency to hallucinate, and canâ€™t recognize when they do so. Reflection-Tuning enables LLMs to recognize their mistakes, and then correct them before committing to an answer. https://t.co/pW78iXSwwb" / X](https://x.com/mattshumer_/status/1831767020519788778)
	- [ ] [mattshumer/Reflection-70B Â· Hugging Face](https://huggingface.co/mattshumer/Reflection-70B)
- [ ] Salesforce xLAM
	- [ ] - [Salesforce AI Research on X: "Introducing the full xLAM family, our groundbreaking suite of Large Action Models! ðŸš€ From the 'Tiny Giant' to industrial powerhouses, xLAM is revolutionizing AI efficiency! #AIResearch #AIEfficiency ðŸ¤— Hugging Face Collection: https://t.co/FTnNVMIXCV ðŸ¤© Research Blog https://t.co/yLcCj1isGx" / X](https://x.com/SFResearch/status/1832117658533134375)

## Videos

- [ ] [Lecture 28: Liger Kernel - Efficient Triton Kernels for LLM Training - YouTube](https://www.youtube.com/watch?v=gWble4FreV4) #triton
	- [ ] int64 addressing slower than int32, need to cast to int64 for large tensors
- [ ] [Cohere For AI - Community Talks: Mostafa Elhoushi & Akshat Shrivastava - YouTube](https://youtu.be/Zl2bBz12JLo?si=vPcqDqR5L35OgeiL)


## Dev
- [ ] [Production-ready Python Docker Containers with uv](https://hynek.me/articles/docker-uv/) #python #uv #docker
- [ ] [CUDA-Free Inference for LLMs | PyTorch](https://pytorch.org/blog/cuda-free-inference-for-llms/) #pytorch
- [ ] [SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org](https://lmsys.org/blog/2024-09-04-sglang-v0-3/)
- [ ] [Advanced Python: Achieving High Performance with Code Generation | by Yonatan Zunger | Medium](https://medium.com/@yonatanzunger/advanced-python-achieving-high-performance-with-code-generation-796b177ec79)

## Random
- [ ] [Ilya Sutskever's SSI Inc raises $1B | Hacker News](https://news.ycombinator.com/item?id=41445413)
- [ ] [Dylan Freedman on X: "The new Qwen2-VL-7B Instruct model gets *100%* accuracy extracting text from this handwritten document. This is the first open weights model (Apache 2.0) that I've seen OCR this accurately. (Thank you @fdaudens for the tip!) https://t.co/AB9r3bKDF0 https://t.co/nAEY7cp1w8" / X](https://x.com/dylfreed/status/1831075759747723709/photo/1)
- [ ] [Fetching Title#xsta](https://medium.com/@yonatanzunger/advanced-python-achieving-high-performance-with-code-generation-796b177ec79)