
## Models
- [ ] [GitHub - deepseek-ai/DeepSeek-Coder-V2: DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/deepseek-ai/DeepSeek-Coder-V2)
- [ ] [Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown](https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1)
	- [ ] #todo distill to SSM / Mamba model
	- [ ] #todo train OCR model on page image to markdown
	- [ ] #todo train html / image to json schema model
- [ ] [\[2409.06666\] LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://arxiv.org/abs/2409.06666)
	- [ ] [ICTNLP/Llama-3.1-8B-Omni Â· Hugging Face](https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni) #speech
- [ ] [mistral-community/pixtral-12b-240910 Â· Hugging Face](https://huggingface.co/mistral-community/pixtral-12b-240910)
- [ ] [x.com](https://x.com/tianyixiong23/status/1834743145877164401)

## Papers
- [ ] [\[2404.03085\] Talaria: Interactively Optimizing Machine Learning Models for Efficient Inference](https://arxiv.org/abs/2404.03085)
	- [ ] [Mycelium â€¢ Graph visualization library](https://apple.github.io/ml-mycelium/)
- [ ] [\[2409.03460\] LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones](https://arxiv.org/abs/2409.03460)
	- [ ] [GitHub - altair199797/LowFormer](https://github.com/altair199797/LowFormer)
- [ ] [\[2409.04431\] Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/abs/2409.04431)
	- [ ] [GitHub - apple/ml-sigmoid-attention](https://github.com/apple/ml-sigmoid-attention) #attention
- [ ] [\[2409.07431\] Synthetic continued pretraining](https://arxiv.org/abs/2409.07431)
- [ ] [\[2409.07146\] Gated Slot Attention for Efficient Linear-Time Sequence Modeling](https://arxiv.org/abs/2409.07146)
- [ ] [\[2409.08239\] Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources](https://arxiv.org/abs/2409.08239) #synthetic-data

## Code
- [ ] [GitHub - kvcache-ai/ktransformers: A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations](https://github.com/kvcache-ai/ktransformers) #triton
- [ ] [GitHub - sustcsonglin/flash-linear-attention: Efficient implementations of state-of-the-art linear attention models in Pytorch and Triton](https://github.com/sustcsonglin/flash-linear-attention) #linear-attention #ssm #triton

## Articles
- [ ] [PDF Retrieval with Vision Language Models | Vespa Blog](https://blog.vespa.ai/retrieval-with-vision-language-models-colpali/)
- [ ] [ColPali: Efficient Document Retrieval with Vision Language Models ðŸ‘€](https://huggingface.co/blog/manu/colpali)

## Videos
- [ ] 

## Other
- [ ] 