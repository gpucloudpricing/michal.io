---
time_modified: 2024-10-11T11:36:37-04:00
time_created: 2024-10-08T15:06:57-04:00
---

## Models
- [ ] [rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model](https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model)
## Papers
- [ ] [\[2410.05258\] Differential Transformer](https://arxiv.org/abs/2410.05258)
- [ ] [\[2410.02884\] LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning](https://arxiv.org/abs/2410.02884)
- [ ] [\[2410.04739\] TableRAG: Million-Token Table Understanding with Language Models](https://arxiv.org/abs/2410.04739)
- [ ] [\[2410.07073\] Pixtral 12B](https://arxiv.org/abs/2410.07073)
- [ ] [\[2410.05993\] Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993)
- [ ] [\[2410.07170\] One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170)
	- [ ] [GitHub - ml-jku/EVA: One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation](https://github.com/ml-jku/EVA)
- [ ] [\[2410.06205\] Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
- [ ] [\[2409.20566\] MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566)
- [ ] [\[2410.05954\] Pyramidal Flow Matching for Efficient Video Generative Modeling](https://arxiv.org/abs/2410.05954)
- [ ] [GitHub - TIGER-AI-Lab/VLM2Vec: This repo contains the code and data for "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks"](https://github.com/TIGER-AI-Lab/VLM2Vec)

## Code
- [ ] 

## Articles
- [ ] [Nixiesearch: running Lucene over S3, and why we’re building a new serverless search engine](https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3) #storage

## Videos
- [ ] [\[M2L 2024\] Mixture of Experts - Diego de Las Casas - YouTube](https://youtu.be/ayguaRDBkgQ?si=_nqrcjHB5G-HdZ_q)
- [ ] [Inference Optimization Tutorial (KDD) - Making models run faster - Part 1 - YouTube](https://youtu.be/uxZ30TB5bqE?si=H3hp7m_KQ9v1Sg8m)
- [ ] [Databases In-Depth – Complete Course - YouTube](https://www.youtube.com/watch?v=pPqazMTzNOM&t=1272s)

## Other


## Tweets

- [Yuchen Jin on X: "It just scales! I trained GPT-2 (350M) using @kellerjordan0's new optimizer and achieved a Fineweb validation loss of 3.05 with just 5.2B tokens—only 52% of the tokens required compared to AdamW. This means you can effectively speed up the GPT-2 training by almost 2X! When https://t.co/4uhCS8TbGq" / X](https://x.com/Yuchenj_UW/status/1844431614404899277)

